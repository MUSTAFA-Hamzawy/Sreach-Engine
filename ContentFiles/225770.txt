[many tasks in natural-language processing and information retrieval involve pairwise comparisons of sentences — for example, sentence similarity detection, paraphrase identification, question-answer entailment, and textual entailment., the most accurate method of sentence comparison is so-called cross-encoding, which maps sentences against each other on a pair-by-pair basis. training cross-encoders, however, requires annotated training data, which is labor intensive to collect., how can we train completely unsupervised models for sentence-pair tasks, eliminating the need for data annotation?, at this year’s international conference on learning representations (iclr), we are presenting an unsupervised sentence-pair model we call a trans-encoder (paper, code), which improves on the prior state of the art by up to 5% on sentence similarity benchmarks., , today, there are basically two paradigms for sentence-pair tasks: cross-encoders and bi-encoders. the choice between the two comes down to the standard trade-off between computational efficiency and performance., cross-encoder. in a cross-encoder, two sequences are concatenated and sent in one pass to the sentence pair model, which is usually built atop a transformer-based language model like bert or roberta. the attention heads of a transformer can directly model which elements of one sequence correlate with which elements of the other, enabling the computation of an accurate classification/relevance score., however, a cross-encoder needs to compute a new encoding for every pair of input sentences, resulting in high computational overhead. cross-encoding is thus impractical for tasks like information retrieval and clustering, which involve massive pairwise sentence comparisons. also, converting pretrained language models (plms) into cross-encoders always requires fine-tuning on annotated data., bi-encoder. by contrast, in a bi-encoder, each sentence is encoded separately and mapped to a common embedding space, where the distances between them can be measured. as the encoded sentences can be cached and reused, bi-encoding is much more efficient, and the outputs of a bi-encoder can be used off-the-shelf as sentence embeddings for downstream tasks., that said, it is well known that in supervised learning, bi-encoders underperform cross-encoders, since they don’t explicitly model interactions between sentences., , in our iclr paper, we ask whether we can leverage the advantages of both bi- and cross-encoders to bootstrap an accurate sentence-pair model in an unsupervised manner., our answer — the trans-encoder — is built on the following intuition: as a starting point, we can use bi-encoder representations to fine-tune a cross-encoder. with its more powerful inter-sentence modeling, the cross-encoder should extract more knowledge from the plms than the bi-encoder can given the same input data. in turn, the more powerful cross-encoder can distill its knowledge back into the bi-encoder, improving the accuracy of the more computationally practical model. we can repeat this cycle to iteratively bootstrap from both the bi- and cross-encoders., specifically, the process of training a trans-encoder is as follows:, step 1. transform plms into effective bi-encoders. to transform existing plms into bi-encoders, we leverage a simple contrastive tuning procedure. given a sentence, we encode it twice, with two different plms. because of dropout — a standard technique in which a fraction of neural-network nodes are randomly dropped during each pass through the training data, to prevent bottlenecks — the two plms will produce slightly different encodings., the bi-encoder is then trained to maximize the similarity of the two almost-identical encodings. this step primes the plms to be good at embedding sequences. details can be found in prior work mirror-bert and simcse., step 2. self-distillation: bi- to cross-encoder. after obtaining a reasonably good bi-encoder from step one, we use it to create training data for a cross-encoder. specifically, we label sentence pairs with the pairwise similarity scores computed by the bi-encoder and use them as training targets for a cross-encoder built on top of a new plm., step 3. self-distillation: cross- to bi-encoder. a natural next step is to distil the extra knowledge gained from the cross-encoder back into bi-encoder form, which is more useful for downstream tasks. more important, a better bi-encoder can produce even more self-labeled data for tuning the cross-encoder. in this way we can repeat steps two and three, continually bootstrapping the encoder performance., our paper proposes other techniques, such as mutual distillation, to improve our model’s performance. please refer to section 2.4 of the paper for more details., benchmark: a new state-of-the-art for sentence similarity, we experiment with the trans-encoder on seven sentence textual similarity (sts) benchmarks. we observe significant improvements upon previous unsupervised sentence-pair models across all datasets., we also benchmark binary-classification and domain transfer tasks. please refer to section 5 of the paper for more details.][improving unsupervised sentence-pair comparison, method that captures advantages of cross-encoding and bi-encoding improves on predecessors by as much as 5%., conference, related publications, a tale of two encoders, trans-encoder: the best of both worlds, conference, related publications, related content, work with us]improving unsupervised sentence-pair comparison - amazon science[][research areas automated reasoning cloud and systems computer vision conversational ai / natural-language processing economics information and knowledge management machine learning operations research and optimization quantum technologies robotics search and information retrieval security, privacy, and abuse prevention sustainability automated reasoning cloud and systems computer vision conversational ai / natural-language processing economics information and knowledge management machine learning operations research and optimization quantum technologies robotics search and information retrieval security, privacy, and abuse prevention sustainability, automated reasoning, cloud and systems, computer vision, conversational ai / natural-language processing, economics, information and knowledge management, machine learning, operations research and optimization, quantum technologies, robotics, search and information retrieval, security, privacy, and abuse prevention, sustainability, automated reasoning, cloud and systems, computer vision, conversational ai / natural-language processing, economics, information and knowledge management, machine learning, operations research and optimization, quantum technologies, robotics, search and information retrieval, security, privacy, and abuse prevention, sustainability, blog, news and features awards and recognitions awards and recognitions, awards and recognitions, awards and recognitions, publications, conferences, collaborations academics at amazon alexa prize amazon research awards academics at amazon alexa prize amazon research awards, academics at amazon, alexa prize, amazon research awards, academics at amazon, alexa prize, amazon research awards, careers internships working at amazon internships working at amazon, internships, working at amazon, internships, working at amazon, twitter, instagram, youtube, facebook, linkedin, research areas automated reasoning cloud and systems computer vision conversational ai / natural-language processing economics information and knowledge management machine learning operations research and optimization quantum technologies robotics search and information retrieval security, privacy, and abuse prevention sustainability automated reasoning cloud and systems computer vision conversational ai / natural-language processing economics information and knowledge management machine learning operations research and optimization quantum technologies robotics search and information retrieval security, privacy, and abuse prevention sustainability, automated reasoning, cloud and systems, computer vision, conversational ai / natural-language processing, economics, information and knowledge management, machine learning, operations research and optimization, quantum technologies, robotics, search and information retrieval, security, privacy, and abuse prevention, sustainability, automated reasoning, cloud and systems, computer vision, conversational ai / natural-language processing, economics, information and knowledge management, machine learning, operations research and optimization, quantum technologies, robotics, search and information retrieval, security, privacy, and abuse prevention, sustainability, blog, news and features awards and recognitions awards and recognitions, awards and recognitions, awards and recognitions, publications, conferences, collaborations academics at amazon alexa prize amazon research awards academics at amazon alexa prize amazon research awards, academics at amazon, alexa prize, amazon research awards, academics at amazon, alexa prize, amazon research awards, careers internships working at amazon internships working at amazon, internships, working at amazon, internships, working at amazon, facebook, twitter, linkedin, email, iclr 2022, trans-encoder: unsupervised sentence-pair modelling through self- and mutual-distillations, conversational ai / natural-language processing, unsupervised learning, semi-supervised learning, natural language processing (nlp), iclr 2022, trans-encoder: unsupervised sentence-pair modelling through self- and mutual-distillations, amazon at iclr: graphs, time series, and more larry hardesty april 26, 2022 other paper topics include natural-language processing, dataset optimization, and the limits of existing machine learning techniques. machine learning, improving question-answering models that use data from tables patrick ng february 28, 2022 novel pretraining method enables increases of 5% to 14% on five different evaluation metrics. search and information retrieval, using food images to find cooking recipes amaia salvador june 23, 2021 a new method based on transformers and trained with self-supervised learning achieves state-of-the-art performance. computer vision, about, research areas, blog, news and features, publications, conferences, collaborations, careers, alexa prize, academics, research awards, amazon developer, amazon web services, about amazon, newsletter, rss, twitter, instagram, youtube, facebook, linkedin]